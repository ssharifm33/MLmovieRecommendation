{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582cbd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /Users/samihakkarainen/miniconda3/envs/cs135_env/lib/python3.10/site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/samihakkarainen/miniconda3/envs/cs135_env/lib/python3.10/site-packages (from scikit-surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/samihakkarainen/miniconda3/envs/cs135_env/lib/python3.10/site-packages (from scikit-surprise) (1.10.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-macosx_11_0_arm64.whl size=1103985 sha256=cfdea6f01bb239850d9d92b703308346f1c0886cf9f907d20cce9e8d2811c9fb\n",
      "  Stored in directory: /Users/samihakkarainen/Library/Caches/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "Successfully installed scikit-surprise-1.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf43039",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/samihakkarainen/Desktop/code/cs135/projB-release/src_starter/part2_log_reg.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samihakkarainen/Desktop/code/cs135/projB-release/src_starter/part2_log_reg.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m y \u001b[39m=\u001b[39m full_df[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samihakkarainen/Desktop/code/cs135/projB-release/src_starter/part2_log_reg.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Split data for training and testing\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samihakkarainen/Desktop/code/cs135/projB-release/src_starter/part2_log_reg.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samihakkarainen/Desktop/code/cs135/projB-release/src_starter/part2_log_reg.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Train logistic regression model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samihakkarainen/Desktop/code/cs135/projB-release/src_starter/part2_log_reg.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs135_env/lib/python3.10/site-packages/surprise/model_selection/split.py:348\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(data, test_size, train_size, random_state, shuffle)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_test_split\u001b[39m(\n\u001b[1;32m    317\u001b[0m     data, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, train_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    318\u001b[0m ):\n\u001b[1;32m    319\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Split a dataset into trainset and testset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[39m    See an example in the :ref:`User Guide <train_test_split_example>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39m            parameter. Shuffling is not done in-place. Default is ``True``.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     ss \u001b[39m=\u001b[39m ShuffleSplit(\n\u001b[1;32m    349\u001b[0m         n_splits\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    350\u001b[0m         test_size\u001b[39m=\u001b[39;49mtest_size,\n\u001b[1;32m    351\u001b[0m         train_size\u001b[39m=\u001b[39;49mtrain_size,\n\u001b[1;32m    352\u001b[0m         random_state\u001b[39m=\u001b[39;49mrandom_state,\n\u001b[1;32m    353\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(ss\u001b[39m.\u001b[39msplit(data))\n",
      "File \u001b[0;32m~/miniconda3/envs/cs135_env/lib/python3.10/site-packages/surprise/model_selection/split.py:227\u001b[0m, in \u001b[0;36mShuffleSplit.__init__\u001b[0;34m(self, n_splits, test_size, train_size, random_state, shuffle)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m n_splits \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    225\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_splits = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m should be strictly greater than \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits)\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m \u001b[39mif\u001b[39;00m test_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m test_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtest_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m should be strictly greater than \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_size)\n\u001b[1;32m    230\u001b[0m     )\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m train_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD, accuracy\n",
    "from surprise.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load data\n",
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "\n",
    "# Prepare Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Function to get SVD features\n",
    "def get_features(row, algo, trainset):\n",
    "    try:\n",
    "        uid = trainset.to_inner_uid(row['user_id'])\n",
    "        iid = trainset.to_inner_iid(row['item_id'])\n",
    "        u_features = algo.pu[uid]\n",
    "        i_features = algo.qi[iid]\n",
    "    except:\n",
    "        u_features = [0]*algo.n_factors\n",
    "        i_features = [0]*algo.n_factors\n",
    "\n",
    "    user_meta = [row['age'], row['is_male']]\n",
    "    item_meta = [row['release_year']]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n",
    "\n",
    "# Merge datasets\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "\n",
    "# Create labels for classification\n",
    "full_df['label'] = (full_df['rating'] >= 4.5).astype(int)\n",
    "\n",
    "# Prepare classification dataset\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = full_df['label'].values\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.2, random_state=42)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and calculate AUC\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC Score: {auc}\")\n",
    "\n",
    "# Evaluate SVD model\n",
    "svd_eval = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "print(\"SVD Model Evaluation:\", svd_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca975601",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Item 1572 is not part of the trainset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/surprise/trainset.py:155\u001b[0m, in \u001b[0;36mTrainset.to_inner_iid\u001b[0;34m(self, riid)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw2inner_id_items[riid]\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 1572",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Generate features for the test set\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_ratings_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Use the logistic regression model to predict probabilities\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     60\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Generate features for the test set\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_ratings_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Use the logistic regression model to predict probabilities\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(info, algo, trainset)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Check and retrieve item features if known\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainset\u001b[38;5;241m.\u001b[39mknows_item(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 39\u001b[0m     inner_iid \u001b[38;5;241m=\u001b[39m \u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_inner_iid\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     i_features \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mqi[inner_iid]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Incorporate additional metadata if available\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/surprise/trainset.py:157\u001b[0m, in \u001b[0;36mTrainset.to_inner_iid\u001b[0;34m(self, riid)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw2inner_id_items[riid]\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mItem \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(riid) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is not part of the trainset.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Item 1572 is not part of the trainset."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load datasets\n",
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "# Merge datasets for features generation\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "test_ratings_df = test_ratings_df.merge(users_df, on='user_id', how='left').merge(movies_df, on='item_id', how='left')\n",
    "\n",
    "# Prepare Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(full_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Define a function to extract features\n",
    "def get_features(info, algo, trainset):\n",
    "    n_factors = algo.n_factors\n",
    "    default_features = np.zeros(n_factors)\n",
    "    u_features = default_features.copy()\n",
    "    i_features = default_features.copy()\n",
    "\n",
    "    # Check and retrieve user features if known\n",
    "    if trainset.knows_user(info['user_id']):\n",
    "        inner_uid = trainset.to_inner_uid(info['user_id'])\n",
    "        u_features = algo.pu[inner_uid]\n",
    "\n",
    "    # Check and retrieve item features if known\n",
    "    if trainset.knows_item(info['item_id']):\n",
    "        inner_iid = trainset.to_inner_iid(info['item_id'])\n",
    "        i_features = algo.qi[inner_iid]\n",
    "\n",
    "    # Incorporate additional metadata if available\n",
    "    user_meta = [info.get('age', 0), info.get('is_male', 0)]\n",
    "    item_meta = [info.get('release_year', 0)]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate features for the training set\n",
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = (full_df['rating'] >= 4.5).astype(int)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "# Generate features for the test set\n",
    "test_ratings_df['features'] = test_ratings_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "X_test = np.stack(test_ratings_df['features'].values)\n",
    "\n",
    "# Use the logistic regression model to predict probabilities\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "test_ratings_df['predicted_rating'] = y_proba\n",
    "\n",
    "# Save the predictions to a text file\n",
    "test_ratings_df[['user_id', 'item_id', 'predicted_rating']].to_csv('predicted_leaderboard_ratings.txt', index=False, sep='\\t')\n",
    "print(\"Predictions saved to 'predicted_leaderboard_ratings.txt'.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c017f1df2ec84156215240b0762243d8e283bbae2bf0907730c1f388e73998cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
