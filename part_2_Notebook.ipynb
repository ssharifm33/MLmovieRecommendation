{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "307ed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "115d3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89992, 3)\n",
      "(943, 4)\n",
      "(1681, 4)\n",
      "(89992, 9)\n"
     ]
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "\n",
    "print(ratings_df.shape)\n",
    "print(users_df.shape)\n",
    "print(movies_df.shape)\n",
    "\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "test_ratings_df = test_ratings_df.merge(users_df, on='user_id', how='left').merge(movies_df, on='item_id', how='left')\n",
    "\n",
    "\n",
    "# Check the new shape of the merged dataframe to ensure completeness\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9b0d21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(full_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a8c8c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user factors matrix: (943, 100)\n",
      "Shape of item factors matrix: (1662, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Number of latent factors\n",
    "n_factors = algo.n_factors\n",
    "\n",
    "# Dimensions of the user factors matrix\n",
    "n_users = algo.trainset.n_users\n",
    "user_factors_shape = (n_users, n_factors)\n",
    "\n",
    "# Dimensions of the item factors matrix\n",
    "n_items = algo.trainset.n_items\n",
    "item_factors_shape = (n_items, n_factors)\n",
    "\n",
    "print(\"Shape of user factors matrix:\", user_factors_shape)\n",
    "print(\"Shape of item factors matrix:\", item_factors_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a495acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(info, algo, trainset):\n",
    "    default_features = np.zeros(algo.n_factors)\n",
    "    u_features = default_features.copy()\n",
    "    i_features = default_features.copy()\n",
    "\n",
    "    if trainset.knows_user(info['user_id']):\n",
    "        inner_uid = trainset.to_inner_uid(info['user_id'])\n",
    "        u_features = algo.pu[inner_uid]\n",
    "    if trainset.knows_item(info['item_id']):\n",
    "        inner_iid = trainset.to_inner_iid(info['item_id'])\n",
    "        i_features = algo.qi[inner_iid]\n",
    "\n",
    "    user_meta = [info['age'], info['is_male']]\n",
    "    item_meta = [info['release_year']]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d0ca71e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m full_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: get_features(row, algo, trainset), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfull_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      5\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = full_df['label'].values\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "\n",
    "# Assuming get_features is defined correctly\n",
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "\n",
    "# Check for any NaNs in features just in case\n",
    "if full_df['features'].isnull().any():\n",
    "    raise ValueError(\"NaN values found in features. Check get_features implementation.\")\n",
    "\n",
    "# Create labels for classification\n",
    "full_df['label'] = (full_df['rating'] >= 4.5).astype(int)\n",
    "\n",
    "# Prepare classification dataset\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = full_df['label'].values\n",
    "\n",
    "# Split data for training and testing with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, check model convergence\n",
    "if not clf.n_iter_ < clf.max_iter:\n",
    "    print(\"Logistic regression did not converge. Consider increasing max_iter or adjusting other parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89992, 3)\n",
      "(943, 4)\n",
      "(1681, 4)\n",
      "(89992, 9)\n"
     ]
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "\n",
    "print(ratings_df.shape)\n",
    "print(users_df.shape)\n",
    "print(movies_df.shape)\n",
    "\n",
    "full_df = ratings_df.merge(users_df, left_on='user_id', right_on='user_id', how='inner')\n",
    "full_df = full_df.merge(movies_df, left_on='item_id', right_on='item_id', how='inner')\n",
    "\n",
    "# Check the new shape of the merged dataframe to ensure completeness\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(full_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user factors matrix: (943, 100)\n",
      "Shape of item factors matrix: (1662, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Number of latent factors\n",
    "n_factors = algo.n_factors\n",
    "\n",
    "# Dimensions of the user factors matrix\n",
    "n_users = algo.trainset.n_users\n",
    "user_factors_shape = (n_users, n_factors)\n",
    "\n",
    "# Dimensions of the item factors matrix\n",
    "n_items = algo.trainset.n_items\n",
    "item_factors_shape = (n_items, n_factors)\n",
    "\n",
    "print(\"Shape of user factors matrix:\", user_factors_shape)\n",
    "print(\"Shape of item factors matrix:\", item_factors_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(info, algo, trainset):\n",
    "    default_features = np.zeros(algo.n_factors)\n",
    "    u_features = default_features.copy()\n",
    "    i_features = default_features.copy()\n",
    "\n",
    "    if trainset.knows_user(info['user_id']):\n",
    "        inner_uid = trainset.to_inner_uid(info['user_id'])\n",
    "        u_features = algo.pu[inner_uid]\n",
    "    if trainset.knows_item(info['item_id']):\n",
    "        inner_iid = trainset.to_inner_iid(info['item_id'])\n",
    "        i_features = algo.qi[inner_iid]\n",
    "\n",
    "    user_meta = [info['age'], info['is_male']]\n",
    "    item_meta = [info['release_year']]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e90c49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "\n",
    "# Assuming get_features is defined correctly\n",
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "\n",
    "# Check for any NaNs in features just in case\n",
    "if full_df['features'].isnull().any():\n",
    "    raise ValueError(\"NaN values found in features. Check get_features implementation.\")\n",
    "\n",
    "# Create labels for classification\n",
    "full_df['label'] = (full_df['rating'] >= 4.5).astype(int)\n",
    "\n",
    "# Prepare classification dataset\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = full_df['label'].values\n",
    "\n",
    "# Split data for training and testing with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, check model convergence\n",
    "if not clf.n_iter_ < clf.max_iter:\n",
    "    print(\"Logistic regression did not converge. Consider increasing max_iter or adjusting other parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6874242506426829\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88     14186\n",
      "           1       0.54      0.07      0.12      3813\n",
      "\n",
      "    accuracy                           0.79     17999\n",
      "   macro avg       0.67      0.53      0.50     17999\n",
      "weighted avg       0.74      0.79      0.72     17999\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13959   227]\n",
      " [ 3545   268]]\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9460  0.9400  0.9419  0.9419  0.9412  0.9422  0.0020  \n",
      "MAE (testset)     0.7443  0.7418  0.7448  0.7420  0.7443  0.7434  0.0013  \n",
      "Fit time          0.58    0.56    0.51    0.53    0.55    0.55    0.02    \n",
      "Test time         0.05    0.05    0.05    0.23    0.05    0.08    0.07    \n",
      "SVD Model Evaluation: {'test_rmse': array([0.94598329, 0.94000225, 0.94188795, 0.94191631, 0.94122541]), 'test_mae': array([0.74426403, 0.74175968, 0.74476644, 0.74195817, 0.74430577]), 'fit_time': (0.5840139389038086, 0.5564937591552734, 0.5107252597808838, 0.5315799713134766, 0.5485568046569824), 'test_time': (0.04686093330383301, 0.0466151237487793, 0.046662092208862305, 0.23178815841674805, 0.046862125396728516)}\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and calculate AUC\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC Score: {auc}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluate SVD model\n",
    "svd_eval = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "print(\"SVD Model Evaluation:\", svd_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af40a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(user_id, item_pool, model, N=10):\n",
    "    known_items = []\n",
    "    items_features = []\n",
    "\n",
    "    # Collect features of known items\n",
    "    for item in item_pool:\n",
    "        if trainset.knows_item(item):\n",
    "            known_items.append(item)\n",
    "            info = {\n",
    "                'user_id': user_id,\n",
    "                'item_id': item,\n",
    "                'age': users_df.loc[users_df['user_id'] == user_id, 'age'].iloc[0],\n",
    "                'is_male': users_df.loc[users_df['user_id'] == user_id, 'is_male'].iloc[0],\n",
    "                'release_year': movies_df.loc[movies_df['item_id'] == item, 'release_year'].iloc[0]\n",
    "            }\n",
    "            features = get_features(info, algo, trainset)\n",
    "            items_features.append(features)\n",
    "        else:\n",
    "            print(f\"Item {item} is not known to the training set and will be skipped.\")\n",
    "\n",
    "    # Handle cases where no known items are found\n",
    "    if not items_features:\n",
    "        print(\"Fallback to popular items.\")\n",
    "        # Select top N popular items\n",
    "        popular_items = ratings_df['item_id'].value_counts().head(N).index.tolist()\n",
    "        return popular_items, [np.nan]*N  # No probabilities available for these items\n",
    "\n",
    "    items_features = np.array(items_features)\n",
    "    probabilities = model.predict_proba(items_features)[:, 1]\n",
    "    top_indices = np.argsort(probabilities)[-N:]\n",
    "\n",
    "    recommended_items = [known_items[i] for i in top_indices if i < len(known_items)]\n",
    "    filtered_probabilities = [probabilities[i] for i in top_indices if i < len(known_items)]\n",
    "\n",
    "    return recommended_items, filtered_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d97be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while making recommendations: Item 709 is not part of the trainset.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of recommendations\n",
    "item_pool = movies_df['item_id'].unique()\n",
    "try:\n",
    "    recommended_items, scores = recommend_items(4, item_pool, clf, N=5)\n",
    "    print(\"Recommended Items:\", recommended_items)\n",
    "    print(\"Scores:\", scores)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while making recommendations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file using pandas\n",
    "test_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "# Convert DataFrame to list of tuples\n",
    "pairs = list(test_df.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6874242506426829\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88     14186\n",
      "           1       0.54      0.07      0.12      3813\n",
      "\n",
      "    accuracy                           0.79     17999\n",
      "   macro avg       0.67      0.53      0.50     17999\n",
      "weighted avg       0.74      0.79      0.72     17999\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13959   227]\n",
      " [ 3545   268]]\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9460  0.9400  0.9419  0.9419  0.9412  0.9422  0.0020  \n",
      "MAE (testset)     0.7443  0.7418  0.7448  0.7420  0.7443  0.7434  0.0013  \n",
      "Fit time          0.58    0.56    0.51    0.53    0.55    0.55    0.02    \n",
      "Test time         0.05    0.05    0.05    0.23    0.05    0.08    0.07    \n",
      "SVD Model Evaluation: {'test_rmse': array([0.94598329, 0.94000225, 0.94188795, 0.94191631, 0.94122541]), 'test_mae': array([0.74426403, 0.74175968, 0.74476644, 0.74195817, 0.74430577]), 'fit_time': (0.5840139389038086, 0.5564937591552734, 0.5107252597808838, 0.5315799713134766, 0.5485568046569824), 'test_time': (0.04686093330383301, 0.0466151237487793, 0.046662092208862305, 0.23178815841674805, 0.046862125396728516)}\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and calculate AUC\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC Score: {auc}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluate SVD model\n",
    "svd_eval = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "print(\"SVD Model Evaluation:\", svd_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af40a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(user_id, item_pool, model, N=10):\n",
    "    known_items = []\n",
    "    items_features = []\n",
    "\n",
    "    # Collect features of known items\n",
    "    for item in item_pool:\n",
    "        if trainset.knows_item(item):\n",
    "            known_items.append(item)\n",
    "            info = { \n",
    "                'user_id': user_id,\n",
    "                'item_id': item,\n",
    "                'age': users_df.loc[users_df['user_id'] == user_id, 'age'].iloc[0],\n",
    "                'is_male': users_df.loc[users_df['user_id'] == user_id, 'is_male'].iloc[0],\n",
    "                'release_year': movies_df.loc[movies_df['item_id'] == item, 'release_year'].iloc[0]\n",
    "            }\n",
    "            features = get_features(info, algo, trainset)\n",
    "            items_features.append(features)\n",
    "        else:\n",
    "            print(f\"Item {item} is not known to the training set and will be skipped.\")\n",
    "\n",
    "    # Handle cases where no known items are found\n",
    "    if not items_features:\n",
    "        print(\"Fallback to popular items.\")\n",
    "        # Select top N popular items\n",
    "        popular_items = ratings_df['item_id'].value_counts().head(N).index.tolist()\n",
    "        return popular_items, [np.nan]*N  # No probabilities available for these items\n",
    "\n",
    "    items_features = np.array(items_features)\n",
    "    probabilities = model.predict_proba(items_features)[:, 1]\n",
    "    top_indices = np.argsort(probabilities)[-N:]\n",
    "\n",
    "    recommended_items = [known_items[i] for i in top_indices if i < len(known_items)]\n",
    "    filtered_probabilities = [probabilities[i] for i in top_indices if i < len(known_items)]\n",
    "\n",
    "    return recommended_items, filtered_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d97be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while making recommendations: Item 709 is not part of the trainset.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of recommendations\n",
    "item_pool = movies_df['item_id'].unique()\n",
    "try:\n",
    "    recommended_items, scores = recommend_items(4, item_pool, clf, N=5)\n",
    "    print(\"Recommended Items:\", recommended_items)\n",
    "    print(\"Scores:\", scores)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while making recommendations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ba06f1cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Item 1572 is not part of the trainset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/surprise/trainset.py:155\u001b[0m, in \u001b[0;36mTrainset.to_inner_iid\u001b[0;34m(self, riid)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw2inner_id_items[riid]\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 1572",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Generate features for the test set\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_ratings_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Predict with logistic regression\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   9567\u001b[0m )\n\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[127], line 53\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     50\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Generate features for the test set\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_ratings_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Predict with logistic regression\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[127], line 35\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(info, algo, trainset)\u001b[0m\n\u001b[1;32m     33\u001b[0m     u_features \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mpu[inner_uid]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainset\u001b[38;5;241m.\u001b[39mknows_item(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 35\u001b[0m     inner_iid \u001b[38;5;241m=\u001b[39m \u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_inner_iid\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitem_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     i_features \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mqi[inner_iid]\n\u001b[1;32m     38\u001b[0m user_meta \u001b[38;5;241m=\u001b[39m [info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m], info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_male\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/surprise/trainset.py:157\u001b[0m, in \u001b[0;36mTrainset.to_inner_iid\u001b[0;34m(self, riid)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw2inner_id_items[riid]\n\u001b[1;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mItem \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(riid) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is not part of the trainset.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Item 1572 is not part of the trainset."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load datasets\n",
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "# Merge datasets for features generation\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "test_ratings_df = test_ratings_df.merge(users_df, on='user_id', how='left').merge(movies_df, on='item_id', how='left')\n",
    "\n",
    "# Prepare Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(full_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Define a function to extract features\n",
    "def get_features(info, algo, trainset):\n",
    "    default_features = np.zeros(algo.n_factors)\n",
    "    u_features = default_features.copy()\n",
    "    i_features = default_features.copy()\n",
    "\n",
    "    if trainset.knows_user(info['user_id']):\n",
    "        inner_uid = trainset.to_inner_uid(info['user_id'])\n",
    "        u_features = algo.pu[inner_uid]\n",
    "    if trainset.knows_item(info['item_id']):\n",
    "        inner_iid = trainset.to_inner_iid(info['item_id'])\n",
    "        i_features = algo.qi[inner_iid]\n",
    "\n",
    "    user_meta = [info['age'], info['is_male']]\n",
    "    item_meta = [info['release_year']]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n",
    "\n",
    "# Generate features for the training set\n",
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = (full_df['rating'] >= 4.5).astype(int)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Generate features for the test set\n",
    "test_ratings_df['features'] = test_ratings_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "X_test = np.stack(test_ratings_df['features'].values)\n",
    "\n",
    "# Predict with logistic regression\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "test_ratings_df['predicted_rating'] = y_proba\n",
    "\n",
    "# Output predictions\n",
    "print(test_ratings_df[['user_id', 'item_id', 'predicted_rating']])\n",
    "\n",
    "# Evaluate SVD model using cross-validation\n",
    "svd_eval = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "print(\"SVD Model Evaluation:\", svd_eval)\n",
    "\n",
    "# Recommendation function (remains the same)\n",
    "def recommend_items(user_id, item_pool, model, N=10):\n",
    "    known_items = []\n",
    "    items_features = []\n",
    "\n",
    "    # Collect features of known items\n",
    "    for item in item_pool:\n",
    "        if trainset.knows_item(item):\n",
    "            known_items.append(item)\n",
    "            info = { \n",
    "                'user_id': user_id,\n",
    "                'item_id': item,\n",
    "                'age': users_df.loc[users_df['user_id'] == user_id, 'age'].iloc[0],\n",
    "                'is_male': users_df.loc[users_df['user_id'] == user_id, 'is_male'].iloc[0],\n",
    "                'release_year': movies_df.loc[movies_df['item_id'] == item, 'release_year'].iloc[0]\n",
    "            }\n",
    "            features = get_features(info, algo, trainset)\n",
    "            items_features.append(features)\n",
    "        else:\n",
    "            print(f\"Item {item} is not known to the training set and will be skipped.\")\n",
    "\n",
    "    # Handle cases where no known items are found\n",
    "    if not items_features:\n",
    "        print(\"Fallback to popular items.\")\n",
    "        # Select top N popular items\n",
    "        popular_items = ratings_df['item_id'].value_counts().head(N).index.tolist()\n",
    "        return popular_items, [np.nan]*N  # No probabilities available for these items\n",
    "\n",
    "    items_features = np.array(items_features)\n",
    "    probabilities = model.predict_proba(items_features)[:, 1]\n",
    "    top_indices = np.argsort(probabilities)[-N:]\n",
    "    recommended_items = [known_items[i] for i in top_indices if i < len(known_items)]\n",
    "    filtered_probabilities = [probabilities[i] for i in top_indices if i < len(known_items)]\n",
    "\n",
    "    return recommended_items, filtered_probabilities\n",
    "\n",
    "# Example usage of recommendations\n",
    "item_pool = movies_df['item_id'].unique()\n",
    "try:\n",
    "    recommended_items, scores = recommend_items(4, item_pool, clf, N=5)\n",
    "    print(\"Recommended Items:\", recommended_items)\n",
    "    print(\"Scores:\", scores)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while making recommendations: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ed876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89992, 3)\n",
      "(943, 4)\n",
      "(1681, 4)\n",
      "(89992, 9)\n"
     ]
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "\n",
    "print(ratings_df.shape)\n",
    "print(users_df.shape)\n",
    "print(movies_df.shape)\n",
    "\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "test_ratings_df = test_ratings_df.merge(users_df, on='user_id', how='left').merge(movies_df, on='item_id', how='left')\n",
    "\n",
    "\n",
    "# Check the new shape of the merged dataframe to ensure completeness\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(full_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of user factors matrix: (943, 100)\n",
      "Shape of item factors matrix: (1662, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Number of latent factors\n",
    "n_factors = algo.n_factors\n",
    "\n",
    "# Dimensions of the user factors matrix\n",
    "n_users = algo.trainset.n_users\n",
    "user_factors_shape = (n_users, n_factors)\n",
    "\n",
    "# Dimensions of the item factors matrix\n",
    "n_items = algo.trainset.n_items\n",
    "item_factors_shape = (n_items, n_factors)\n",
    "\n",
    "print(\"Shape of user factors matrix:\", user_factors_shape)\n",
    "print(\"Shape of item factors matrix:\", item_factors_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(info, algo, trainset):\n",
    "    default_features = np.zeros(algo.n_factors)\n",
    "    u_features = default_features.copy()\n",
    "    i_features = default_features.copy()\n",
    "\n",
    "    if trainset.knows_user(info['user_id']):\n",
    "        inner_uid = trainset.to_inner_uid(info['user_id'])\n",
    "        u_features = algo.pu[inner_uid]\n",
    "    if trainset.knows_item(info['item_id']):\n",
    "        inner_iid = trainset.to_inner_iid(info['item_id'])\n",
    "        i_features = algo.qi[inner_iid]\n",
    "\n",
    "    user_meta = [info['age'], info['is_male']]\n",
    "    item_meta = [info['release_year']]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "\n",
    "# Assuming get_features is defined correctly\n",
    "full_df['features'] = full_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "\n",
    "# Check for any NaNs in features just in case\n",
    "if full_df['features'].isnull().any():\n",
    "    raise ValueError(\"NaN values found in features. Check get_features implementation.\")\n",
    "\n",
    "# Create labels for classification\n",
    "full_df['label'] = (full_df['rating'] >= 4.5).astype(int)\n",
    "\n",
    "# Prepare classification dataset\n",
    "X = np.stack(full_df['features'].values)\n",
    "y = full_df['label'].values\n",
    "\n",
    "# Split data for training and testing with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, stratify=y)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Optionally, check model convergence\n",
    "if not clf.n_iter_ < clf.max_iter:\n",
    "    print(\"Logistic regression did not converge. Consider increasing max_iter or adjusting other parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6874242506426829\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88     14186\n",
      "           1       0.54      0.07      0.12      3813\n",
      "\n",
      "    accuracy                           0.79     17999\n",
      "   macro avg       0.67      0.53      0.50     17999\n",
      "weighted avg       0.74      0.79      0.72     17999\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13959   227]\n",
      " [ 3545   268]]\n",
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9460  0.9400  0.9419  0.9419  0.9412  0.9422  0.0020  \n",
      "MAE (testset)     0.7443  0.7418  0.7448  0.7420  0.7443  0.7434  0.0013  \n",
      "Fit time          0.58    0.56    0.51    0.53    0.55    0.55    0.02    \n",
      "Test time         0.05    0.05    0.05    0.23    0.05    0.08    0.07    \n",
      "SVD Model Evaluation: {'test_rmse': array([0.94598329, 0.94000225, 0.94188795, 0.94191631, 0.94122541]), 'test_mae': array([0.74426403, 0.74175968, 0.74476644, 0.74195817, 0.74430577]), 'fit_time': (0.5840139389038086, 0.5564937591552734, 0.5107252597808838, 0.5315799713134766, 0.5485568046569824), 'test_time': (0.04686093330383301, 0.0466151237487793, 0.046662092208862305, 0.23178815841674805, 0.046862125396728516)}\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and calculate AUC\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC Score: {auc}\")\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluate SVD model\n",
    "svd_eval = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "print(\"SVD Model Evaluation:\", svd_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af40a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(user_id, item_pool, model, N=10):\n",
    "    known_items = []\n",
    "    items_features = []\n",
    "\n",
    "    # Collect features of known items\n",
    "    for item in item_pool:\n",
    "        if trainset.knows_item(item):\n",
    "            known_items.append(item)\n",
    "            info = { \n",
    "                'user_id': user_id,\n",
    "                'item_id': item,\n",
    "                'age': users_df.loc[users_df['user_id'] == user_id, 'age'].iloc[0],\n",
    "                'is_male': users_df.loc[users_df['user_id'] == user_id, 'is_male'].iloc[0],\n",
    "                'release_year': movies_df.loc[movies_df['item_id'] == item, 'release_year'].iloc[0]\n",
    "            }\n",
    "            features = get_features(info, algo, trainset)\n",
    "            items_features.append(features)\n",
    "        else:\n",
    "            print(f\"Item {item} is not known to the training set and will be skipped.\")\n",
    "\n",
    "    # Handle cases where no known items are found\n",
    "    if not items_features:\n",
    "        print(\"Fallback to popular items.\")\n",
    "        # Select top N popular items\n",
    "        popular_items = ratings_df['item_id'].value_counts().head(N).index.tolist()\n",
    "        return popular_items, [np.nan]*N  # No probabilities available for these items\n",
    "\n",
    "    items_features = np.array(items_features)\n",
    "    probabilities = model.predict_proba(items_features)[:, 1]\n",
    "    top_indices = np.argsort(probabilities)[-N:]\n",
    "\n",
    "    recommended_items = [known_items[i] for i in top_indices if i < len(known_items)]\n",
    "    filtered_probabilities = [probabilities[i] for i in top_indices if i < len(known_items)]\n",
    "\n",
    "    return recommended_items, filtered_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d97be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while making recommendations: Item 709 is not part of the trainset.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of recommendations\n",
    "item_pool = movies_df['item_id'].unique()\n",
    "try:\n",
    "    recommended_items, scores = recommend_items(4, item_pool, clf, N=5)\n",
    "    print(\"Recommended Items:\", recommended_items)\n",
    "    print(\"Scores:\", scores)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while making recommendations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06f1cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'age'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n",
      "\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n",
      "\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: 'age'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[126], line 42\u001b[0m\n",
      "\u001b[1;32m     39\u001b[0m algo\u001b[38;5;241m.\u001b[39mfit(trainset)\n",
      "\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Generate features for the test set\u001b[39;00m\n",
      "\u001b[0;32m---> 42\u001b[0m test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_ratings_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     43\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Predict with logistic regression\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   9557\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n",
      "\u001b[1;32m   9559\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n",
      "\u001b[1;32m   9560\u001b[0m     \u001b[39mself\u001b[39m,\n",
      "\u001b[1;32m   9561\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   9566\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n",
      "\u001b[1;32m   9567\u001b[0m )\n",
      "\u001b[0;32m-> 9568\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    761\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n",
      "\u001b[1;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n",
      "\u001b[0;32m--> 764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;32m--> 891\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n",
      "\u001b[1;32m    893\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n",
      "\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    904\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[1;32m    905\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n",
      "\u001b[1;32m    906\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n",
      "\u001b[0;32m--> 907\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n",
      "\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n",
      "\u001b[1;32m    909\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n",
      "\u001b[1;32m    910\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n",
      "\u001b[1;32m    911\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\n",
      "Cell \u001b[0;32mIn[126], line 42\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n",
      "\u001b[1;32m     39\u001b[0m algo\u001b[38;5;241m.\u001b[39mfit(trainset)\n",
      "\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Generate features for the test set\u001b[39;00m\n",
      "\u001b[0;32m---> 42\u001b[0m test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_ratings_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m     43\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(test_ratings_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Predict with logistic regression\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[126], line 24\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(info, algo, trainset)\u001b[0m\n",
      "\u001b[1;32m     21\u001b[0m     inner_iid \u001b[38;5;241m=\u001b[39m trainset\u001b[38;5;241m.\u001b[39mto_inner_iid(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;32m     22\u001b[0m     i_features \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mqi[inner_iid]\n",
      "\u001b[0;32m---> 24\u001b[0m user_meta \u001b[38;5;241m=\u001b[39m [\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_male\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;32m     25\u001b[0m item_meta \u001b[38;5;241m=\u001b[39m [info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelease_year\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;32m     26\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([u_features, i_features, user_meta, item_meta])\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n",
      "\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n",
      "\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n",
      "\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n",
      "\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n",
      "\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n",
      "\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n",
      "\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n",
      "\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n",
      "\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "\n",
      "File \u001b[0;32m~/micromamba/envs/cs135_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n",
      "\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n",
      "\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n",
      "\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n",
      "\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: 'age'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Reader, Dataset, SVD\n",
    "\n",
    "# Load datasets\n",
    "ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "users_df = pd.read_csv('../data_movie_lens_100k/user_info.csv')\n",
    "movies_df = pd.read_csv('../data_movie_lens_100k/movie_info.csv')\n",
    "test_ratings_df = pd.read_csv('../data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "# Define a function to extract features\n",
    "def get_features(info, algo, trainset):\n",
    "    default_features = np.zeros(algo.n_factors)\n",
    "    u_features = default_features.copy()\n",
    "    i_features = default_features.copy()\n",
    "\n",
    "    if trainset.knows_user(info['user_id']):\n",
    "        inner_uid = trainset.to_inner_uid(info['user_id'])\n",
    "        u_features = algo.pu[inner_uid]\n",
    "    if trainset.knows_item(info['item_id']):\n",
    "        inner_iid = trainset.to_inner_iid(info['item_id'])\n",
    "        i_features = algo.qi[inner_iid]\n",
    "\n",
    "    user_meta = [info['age'], info['is_male']]\n",
    "    item_meta = [info['release_year']]\n",
    "    features = np.concatenate([u_features, i_features, user_meta, item_meta])\n",
    "    return features\n",
    "\n",
    "# Merge datasets for features generation\n",
    "full_df = ratings_df.merge(users_df, on='user_id').merge(movies_df, on='item_id')\n",
    "\n",
    "# Prepare Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(full_df[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Train SVD model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generate features for the test set\n",
    "test_ratings_df['features'] = test_ratings_df.apply(lambda row: get_features(row, algo, trainset), axis=1)\n",
    "X_test = np.stack(test_ratings_df['features'].values)\n",
    "\n",
    "# Predict with logistic regression\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "test_ratings_df['predicted_rating'] = y_proba\n",
    "\n",
    "# Save the predictions to a text file\n",
    "output_file_path = '/mnt/data/ratings_leaderboard_predictions.txt'\n",
    "test_ratings_df[['user_id', 'item_id', 'predicted_rating']].to_csv(output_file_path, index=False, header=True, sep='\\t')\n",
    "\n",
    "output_file_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c017f1df2ec84156215240b0762243d8e283bbae2bf0907730c1f388e73998cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
